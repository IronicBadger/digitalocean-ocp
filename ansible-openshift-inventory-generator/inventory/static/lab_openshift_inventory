# This is an example of an OpenShift-Ansible host inventory

# Create an OSEv3 group that contains the masters and nodes groups
[OSEv3:children]
masters
nodes
infra_nodes
etcd
#lb
#nfs

# Full comments available below:
# https://github.com/openshift/openshift-ansible/blob/master/inventory/hosts.example
[OSEv3:vars]

### Ansible configuration
ansible_user=root
ansible_become=False
ansible_ssh_private_key_file=/root/roles/lab-ocp-inventory-gen/files/ocp_lab.pem
debug_level=2

### Openshift deployment configuration
openshift_deployment_type=origin
containerized=True
openshift_release=v3.7

### General configuration
#openshift_image_tag=0
#openshift_pkg_version=-3.7.0
openshift_install_examples=True
#openshift_master_image_policy_config={"maxImagesBulkImportedPerRepository": 3, "disableScheduledImport": true}

### Configure master API rate limits for external clients
openshift_master_external_ratelimit_qps=200
openshift_master_external_ratelimit_burst=400
# Configure master API rate limits for loopback clients
#openshift_master_loopback_ratelimit_qps=300
#openshift_master_loopback_ratelimit_burst=600

### Docker Configuration
openshift_docker_additional_registries=registry.example.com
#openshift_docker_insecure_registries=registry.example.com
#openshift_docker_blocked_registries=registry.hacker.com
#openshift_docker_disable_push_dockerhub=True
openshift_docker_options=-l warn --log-opt max-size=50m --ipv6=false

### Auth
openshift_master_identity_providers=[{'name': 'allow_all', 'login': 'true', 'challenge': 'true', 'kind': 'AllowAllPasswordIdentityProvider'}]

### Project Configuration
osm_project_request_message=''
osm_project_request_template=''
osm_mcs_allocator_range='s0:/2'
osm_mcs_labels_per_project=5
osm_uid_allocator_range='1000000000-1999999999/10000'

### Cockpit configuration
osm_use_cockpit=true
osm_cockpit_plugins=['cockpit-kubernetes']

### HA method

openshift_master_cluster_hostname=ocp.ktz.cloud
openshift_master_cluster_public_hostname=ocp.ktz.cloud
openshift_master_default_subdomain=apps.ktz.cloud

### Override default node selector
#osm_default_node_selector='region=primary'

### Default storage plugin dependencies to install
#osn_storage_plugin_deps=['ceph','glusterfs','iscsi']

### Router Options

#openshift_hosted_router_selector='region=infra'
#openshift_hosted_router_replicas=2
#openshift_hosted_router_force_subdomain='${name}-${namespace}.apps.example.com'
#openshift_hosted_router_certificate={"certfile": "/path/to/router.crt", "keyfile": "/path/to/router.key", "cafile": "/path/to/router-ca.crt"}
#openshift_hosted_manage_router=true
#openshift_hosted_routers=[{'name': 'router1', 'certificate': {'certfile': '/path/to/certificate/abc.crt', 'keyfile': '/path/to/certificate/abc.key', 'cafile': '/path/to/certificate/ca.crt'}, 'replicas': 1, 'serviceaccount': 'router', 'namespace': 'default', 'stats_port': 1936, 'edits': [], 'images': 'openshift3/ose-${component}:${version}', 'selector': 'type=router1', 'ports': ['80:80', '443:443']}, {'name': 'router2', 'certificate': {'certfile': '/path/to/certificate/xyz.crt', 'keyfile': '/path/to/certificate/xyz.key', 'cafile': '/path/to/certificate/ca.crt'}, 'replicas': 1, 'serviceaccount': 'router', 'namespace': 'default', 'stats_port': 1936, 'edits': [{'action': 'append', 'key': 'spec.template.spec.containers[0].env', 'value': {'name': 'ROUTE_LABELS', 'value': 'route=external'}}], 'images': 'openshift3/ose-${component}:${version}', 'selector': 'type=router2', 'ports': ['80:80', '443:443']}]

### Registry Options
#openshift_hosted_registry_selector='region=infra'
#openshift_hosted_registry_replicas=2
#openshift_hosted_manage_registry=true

# Registry Storage Options
#
# NFS Host Group
# An NFS volume will be created with path "nfs_directory/volume_name"
# on the host within the [nfs] host group.  For example, the volume
# path using these options would be "/exports/registry".  "exports" is
# is the name of the export served by the nfs server.  "registry" is
# the name of a directory inside of "/exports".
#openshift_hosted_registry_storage_kind=nfs
#openshift_hosted_registry_storage_access_modes=['ReadWriteMany']
# nfs_directory must conform to DNS-1123 subdomain must consist of lower case
# alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character
#openshift_hosted_registry_storage_nfs_directory=/exports
#openshift_hosted_registry_storage_nfs_options='*(rw,root_squash)'
#openshift_hosted_registry_storage_volume_name=registry
#openshift_hosted_registry_storage_volume_size=10Gi
#
# External NFS Host
# NFS volume must already exist with path "nfs_directory/_volume_name" on
# the storage_host. For example, the remote volume path using these
# options would be "nfs.example.com:/exports/registry".  "exports" is
# is the name of the export served by the nfs server.  "registry" is
# the name of a directory inside of "/exports".
#openshift_hosted_registry_storage_kind=nfs
#openshift_hosted_registry_storage_access_modes=['ReadWriteMany']
#openshift_hosted_registry_storage_host=nfs.example.com
# nfs_directory must conform to DNS-1123 subdomain must consist of lower case
# alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character
#openshift_hosted_registry_storage_nfs_directory=/exports
#openshift_hosted_registry_storage_volume_name=registry
#openshift_hosted_registry_storage_volume_size=10Gi
#
# AWS S3
# S3 bucket must already exist.
#openshift_hosted_registry_storage_kind=nfs
#openshift_hosted_registry_storage_provider=s3
#openshift_hosted_registry_storage_s3_encrypt=False
#openshift_hosted_registry_storage_s3_kmskeyid=aws_kms_key_id
#openshift_hosted_registry_storage_s3_accesskey=aws_access_key_id
#openshift_hosted_registry_storage_s3_secretkey=aws_secret_access_key
#openshift_hosted_registry_storage_s3_bucket=bucket_name
#openshift_hosted_registry_storage_s3_region=bucket_region
#openshift_hosted_registry_storage_s3_chunksize=26214400
#openshift_hosted_registry_storage_s3_rootdirectory=/registry
#openshift_hosted_registry_pullthrough=True
#openshift_hosted_registry_acceptschema2=True
#openshift_hosted_registry_enforcequota=True
#
# Any S3 service (Minio, ExoScale, ...): Basically the same as above
# but with regionendpoint configured
# S3 bucket must already exist.
#openshift_hosted_registry_storage_kind=nfs
#openshift_hosted_registry_storage_provider=s3
#openshift_hosted_registry_storage_s3_accesskey=aws_access_key_id
#openshift_hosted_registry_storage_s3_secretkey=aws_secret_access_key
#openshift_hosted_registry_storage_s3_regionendpoint=https://myendpoint.example.com/
#openshift_hosted_registry_storage_s3_bucket=bucket_name
#openshift_hosted_registry_storage_s3_region=bucket_region
#openshift_hosted_registry_storage_s3_chunksize=26214400
#openshift_hosted_registry_storage_s3_rootdirectory=/registry
#openshift_hosted_registry_pullthrough=True
#openshift_hosted_registry_acceptschema2=True
#openshift_hosted_registry_enforcequota=True
#
# Metrics deployment
# See: https://docs.openshift.com/enterprise/latest/install_config/cluster_metrics.html
#
# By default metrics are not automatically deployed, set this to enable them
#openshift_metrics_install_metrics=true
#
# Storage Options
# If openshift_metrics_storage_kind is unset then metrics will be stored
# in an EmptyDir volume and will be deleted when the cassandra pod terminates.
# Storage options A & B currently support only one cassandra pod which is
# generally enough for up to 1000 pods. Additional volumes can be created
# manually after the fact and metrics scaled per the docs.
#
# Option A - NFS Host Group
# An NFS volume will be created with path "nfs_directory/volume_name"
# on the host within the [nfs] host group.  For example, the volume
# path using these options would be "/exports/metrics".  "exports" is
# is the name of the export served by the nfs server.  "metrics" is
# the name of a directory inside of "/exports".
#openshift_metrics_storage_kind=nfs
#openshift_metrics_storage_access_modes=['ReadWriteOnce']
#openshift_metrics_storage_nfs_directory=/exports
#openshift_metrics_storage_nfs_options='*(rw,root_squash)'
#openshift_metrics_storage_volume_name=metrics
#openshift_metrics_storage_volume_size=10Gi
#openshift_metrics_storage_labels= {'storage': 'metrics'}
#
# Option B - External NFS Host
# NFS volume must already exist with path "nfs_directory/_volume_name" on
# the storage_host. For example, the remote volume path using these
# options would be "nfs.example.com:/exports/metrics".  "exports" is
# is the name of the export served by the nfs server.  "metrics" is
# the name of a directory inside of "/exports".
#openshift_metrics_storage_kind=nfs
#openshift_metrics_storage_access_modes=['ReadWriteOnce']
#openshift_metrics_storage_host=nfs.example.com
#openshift_metrics_storage_nfs_directory=/exports
#openshift_metrics_storage_volume_name=metrics
#openshift_metrics_storage_volume_size=10Gi
#openshift_metrics_storage_labels={'storage': 'metrics'}
#
# Option C - Dynamic -- If openshift supports dynamic volume provisioning for
# your cloud platform use this. (enter dynamic as variable here)
#openshift_metrics_storage_kind=nfs
#
# Other Metrics Options -- Common items you may wish to reconfigure, for the complete
# list of options please see roles/openshift_metrics/README.md
#
# Override metricsPublicURL in the master config for cluster metrics
# Defaults to https://hawkular-metrics.apps.ktz.cloud/hawkular/metrics
# Currently, you may only alter the hostname portion of the url, alterting the
# `/hawkular/metrics` path will break installation of metrics.
#openshift_metrics_hawkular_hostname=hawkular-metrics.example.com
# Configure the prefix and version for the component images
#openshift_metrics_image_prefix=docker.io/openshift/origin-
#openshift_metrics_image_version=v3.7
# when openshift_deployment_type=='openshift-enterprise'
#openshift_metrics_image_prefix=registry.access.redhat.com/openshift3/
#openshift_metrics_image_version=v3.7
#
# StorageClass
# openshift_storageclass_name=gp2
# openshift_storageclass_parameters={'type': 'gp2', 'encrypted': 'false'}
#

# Logging deployment
#
# Currently logging deployment is disabled by default, enable it by setting this
#openshift_logging_install_logging=true
#
# Logging storage config
# Option A - NFS Host Group
# An NFS volume will be created with path "nfs_directory/volume_name"
# on the host within the [nfs] host group.  For example, the volume
# path using these options would be "/exports/logging".  "exports" is
# is the name of the export served by the nfs server.  "logging" is
# the name of a directory inside of "/exports".
#openshift_logging_storage_kind=nfs
#openshift_logging_storage_access_modes=['ReadWriteOnce']
#openshift_logging_storage_nfs_directory=/exports
#openshift_logging_storage_nfs_options='*(rw,root_squash)'
#openshift_logging_storage_volume_name=logging
#openshift_logging_storage_volume_size=10Gi
#openshift_logging_storage_labels={'storage': 'logging'}
#
# Option B - External NFS Host
# NFS volume must already exist with path "nfs_directory/_volume_name" on
# the storage_host. For example, the remote volume path using these
# options would be "nfs.example.com:/exports/logging".  "exports" is
# is the name of the export served by the nfs server.  "logging" is
# the name of a directory inside of "/exports".
#openshift_logging_storage_kind=nfs
#openshift_logging_storage_access_modes=['ReadWriteOnce']
#openshift_logging_storage_host=nfs.example.com
#openshift_logging_storage_nfs_directory=/exports
#openshift_logging_storage_volume_name=logging
#openshift_logging_storage_volume_size=10Gi
#openshift_logging_storage_labels={'storage': 'logging'}
#
# Option C - Dynamic -- If openshift supports dynamic volume provisioning for
# your cloud platform use this.
#openshift_logging_storage_kind=nfs
#
# Option D - none -- Logging will use emptydir volumes which are destroyed when
# pods are deleted
#
# Other Logging Options -- Common items you may wish to reconfigure, for the complete
# list of options please see roles/openshift_logging/README.md
#
# Configure loggingPublicURL in the master config for aggregate logging, defaults
# to kibana.apps.ktz.cloud
#openshift_logging_kibana_hostname=logging.apps.example.com
# Configure the number of elastic search nodes, unless you're using dynamic provisioning
# this value must be 1
#openshift_logging_es_cluster_size=1
# Configure the prefix and version for the component images
#openshift_logging_image_prefix=docker.io/openshift/origin-
#openshift_logging_image_version=v3.7
# when openshift_deployment_type=='openshift-enterprise'
#openshift_logging_image_prefix=registry.access.redhat.com/openshift3/
#openshift_logging_image_version=3.7.0

# Prometheus deployment
#
# Currently prometheus deployment is disabled by default, enable it by setting this
#openshift_hosted_prometheus_deploy=true
#
# Prometheus storage config
# Option A - NFS Host Group
# An NFS volume will be created with path "nfs_directory/volume_name"
# on the host within the [nfs] host group.  For example, the volume
# path using these options would be "/exports/prometheus"
#openshift_prometheus_storage_kind=nfs
#openshift_prometheus_storage_access_modes=['ReadWriteOnce']
#openshift_prometheus_storage_nfs_directory=/exports
#openshift_prometheus_storage_nfs_options='*(rw,root_squash)'
#openshift_prometheus_storage_volume_name=prometheus
#openshift_prometheus_storage_volume_size=10Gi
#openshift_prometheus_storage_labels={'storage': 'prometheus'}
#openshift_prometheus_storage_type='pvc'
#openshift_prometheus_storage_class=glusterfs-storage
# For prometheus-alertmanager
#openshift_prometheus_alertmanager_storage_kind=nfs
#openshift_prometheus_alertmanager_storage_access_modes=['ReadWriteOnce']
#openshift_prometheus_alertmanager_storage_nfs_directory=/exports
#openshift_prometheus_alertmanager_storage_nfs_options='*(rw,root_squash)'
#openshift_prometheus_alertmanager_storage_volume_name=prometheus-alertmanager
#openshift_prometheus_alertmanager_storage_volume_size=10Gi
#openshift_prometheus_alertmanager_storage_labels={'storage': 'prometheus-alertmanager'}
#openshift_prometheus_alertmanager_storage_type='pvc'
#openshift_prometheus_alertmanager_storage_class=glusterfs-storage
# For prometheus-alertbuffer
#openshift_prometheus_alertbuffer_storage_kind=nfs
#openshift_prometheus_alertbuffer_storage_access_modes=['ReadWriteOnce']
#openshift_prometheus_alertbuffer_storage_nfs_directory=/exports
#openshift_prometheus_alertbuffer_storage_nfs_options='*(rw,root_squash)'
#openshift_prometheus_alertbuffer_storage_volume_name=prometheus-alertbuffer
#openshift_prometheus_alertbuffer_storage_volume_size=10Gi
#openshift_prometheus_alertbuffer_storage_labels={'storage': 'prometheus-alertbuffer'}
#openshift_prometheus_alertbuffer_storage_type='pvc'
#openshift_prometheus_alertbuffer_storage_class=glusterfs-storage
#
# Option B - External NFS Host
# NFS volume must already exist with path "nfs_directory/_volume_name" on
# the storage_host. For example, the remote volume path using these
# options would be "nfs.example.com:/exports/prometheus"
#openshift_prometheus_storage_kind=nfs
#openshift_prometheus_storage_access_modes=['ReadWriteOnce']
#openshift_prometheus_storage_host=nfs.example.com
#openshift_prometheus_storage_nfs_directory=/exports
#openshift_prometheus_storage_volume_name=prometheus
#openshift_prometheus_storage_volume_size=10Gi
#openshift_prometheus_storage_labels={'storage': 'prometheus'}
#openshift_prometheus_storage_type='pvc'
#openshift_prometheus_storage_class=glusterfs-storage
# For prometheus-alertmanager
#openshift_prometheus_alertmanager_storage_kind=nfs
#openshift_prometheus_alertmanager_storage_access_modes=['ReadWriteOnce']
#openshift_prometheus_alertmanager_storage_host=nfs.example.com
#openshift_prometheus_alertmanager_storage_nfs_directory=/exports
#openshift_prometheus_alertmanager_storage_volume_name=prometheus-alertmanager
#openshift_prometheus_alertmanager_storage_volume_size=10Gi
#openshift_prometheus_alertmanager_storage_labels={'storage': 'prometheus-alertmanager'}
#openshift_prometheus_alertmanager_storage_type='pvc'
#openshift_prometheus_alertmanager_storage_class=glusterfs-storage
# For prometheus-alertbuffer
#openshift_prometheus_alertbuffer_storage_kind=nfs
#openshift_prometheus_alertbuffer_storage_access_modes=['ReadWriteOnce']
#openshift_prometheus_alertbuffer_storage_host=nfs.example.com
#openshift_prometheus_alertbuffer_storage_nfs_directory=/exports
#openshift_prometheus_alertbuffer_storage_volume_name=prometheus-alertbuffer
#openshift_prometheus_alertbuffer_storage_volume_size=10Gi
#openshift_prometheus_alertbuffer_storage_labels={'storage': 'prometheus-alertbuffer'}
#openshift_prometheus_alertbuffer_storage_type='pvc'
#openshift_prometheus_alertbuffer_storage_class=glusterfs-storage
#
# Option C - none -- Prometheus, alertmanager and alertbuffer will use emptydir volumes
# which are destroyed when pods are deleted

# Configure the multi-tenant SDN plugin (default is 'redhat/openshift-ovs-subnet')
# other option is 'redhat/openshift-ovs-multitenant'
os_sdn_network_plugin_name='redhat/openshift-ovs-multitenant'
# openshift_use_openshift_sdn=False

### Master API and console ports.
openshift_master_api_port=443
openshift_master_console_port=443

### TLS certificates
#openshift_master_ca_certificate={'certfile': '/path/to/ca.crt', 'keyfile': '/path/to/ca.key'}
#openshift_master_overwrite_named_certificates=True
#openshift_master_named_certificates=[{"certfile": "/path/to/custom1.crt", "keyfile": "/path/to/custom1.key", "cafile": "/path/to/custom-ca1.crt"}]

# Session options
openshift_master_session_name=ssn
openshift_master_session_max_seconds=3600

# Configure dnsIP in the node config
#openshift_dns_ip=172.30.0.1

# Configure node kubelet arguments. pods-per-core is valid in OpenShift Origin 1.3 or OpenShift Container Platform 3.3 and later.
#openshift_node_kubelet_args={'pods-per-core': ['10'], 'max-pods': ['250'], 'image-gc-high-threshold': ['85'], 'image-gc-low-threshold': ['80']}

# Configure logrotate scripts
# See: https://github.com/nickhammond/ansible-logrotate
logrotate_scripts=[{"name": "syslog", "path": "/var/log/cron\n/var/log/maillog\n/var/log/messages\n/var/log/secure\n/var/log/spooler\n", "options": ["daily", "rotate 7", "compress", "sharedscripts", "missingok"], "scripts": {"postrotate": "/bin/kill -HUP `cat /var/run/syslogd.pid 2> /dev/null` 2> /dev/null || true"}}]

### Build configuration
openshift_builddefaults_resources_requests_cpu=100m
openshift_builddefaults_resources_requests_memory=256Mi
openshift_builddefaults_resources_limits_cpu=1000m
openshift_builddefaults_resources_limits_memory=512Mi

# Enable service catalog
openshift_enable_service_catalog=True

# Enable template service broker (requires service catalog to be enabled, above)
template_service_broker_install=True

# Configure usage of openshift_clock role.
openshift_clock_enabled=True

# OpenShift Per-Service Environment Variables
openshift_master_api_env_vars={"ENABLE_HTTP2": "true"}
openshift_master_controllers_env_vars={"ENABLE_HTTP2": "true"}
openshift_node_env_vars={"ENABLE_HTTP2": "true"}

# Enable API service auditing
openshift_master_audit_config={"enabled": true, "auditFilePath": "/var/log/audit-ocp.log", "maximumFileRetentionDays": 10, "maximumFileSizeMegabytes": 10, "maximumRetainedFiles": 10}

## Masters

[masters]
174.138.11.213 openshift_hostname=174.138.11.213 openshift_dns_ip=174.138.11.213

[masters:vars]
openshift_node_labels={'purpose': 'master', 'region': 'primary', 'router': 'true'}
openshift_schedulable=false


## ETCD
[etcd]
174.138.11.213 openshift_hostname=174.138.11.213 openshift_dns_ip=174.138.11.213

[etcd:vars]
openshift_node_labels={'purpose': 'etcd', 'region': 'primary'}
openshift_schedulable=false


## Infra nodes
[infra_nodes]
174.138.11.213 openshift_hostname=174.138.11.213 openshift_dns_ip=174.138.11.213

[infra_nodes:vars]
openshift_node_labels={'purpose': 'infra', 'region': 'primary'}


## App nodes
[app_nodes]
174.138.11.191 openshift_hostname=174.138.11.191 openshift_dns_ip=174.138.11.191
174.138.11.192 openshift_hostname=174.138.11.192 openshift_dns_ip=174.138.11.192

[app_nodes:vars]
openshift_node_labels={'purpose': 'work', 'region': 'primary'}

###################################################################################

# Nodes
[nodes:children]
infra_nodes
app_nodes
etcd
masters

[new_masters]


[new_nodes]
